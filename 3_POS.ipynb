{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__POS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(input_data, target, ratio=0.3, rand_state=42):\n",
    "    return train_test_split(input_data, target, test_size=ratio, stratify=target, random_state=rand_state)\n",
    "\n",
    "def apply_grid_search_cv(pipe, param_grid, X_train, y_train, X_test, y_test, print_flag=True, score_matrix=f1_score, n_jobs=-1, cv=5):\n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid, scoring=make_scorer(score_matrix), n_jobs=n_jobs, cv=cv)\n",
    "    t0 = time()\n",
    "    res = grid_search.fit(X_train, y_train)\n",
    "    if print_flag:\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"best params:\")\n",
    "        print(res.best_params_)\n",
    "        print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "        print(\"Test-set score: {:.3f}\".format(grid_search.score(X_test, y_test)))        \n",
    "    return grid_search\n",
    "\n",
    "def save_class_report_cv(grid_search, X_test, y_test, target_names, filename):\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cross_validation_results(grid_search, filename, print_flag=True):\n",
    "    param_keys = list(grid_search.cv_results_[\"params\"][0].keys())\n",
    "    matrix_list = [\"mean_test_score\", \"std_test_score\", \"rank_test_score\", \"mean_fit_time\"] \n",
    "    col = param_keys + matrix_list\n",
    "\n",
    "    cv_results = []\n",
    "    cv_results.append(col)    \n",
    "    \n",
    "    for param, score, std, rank, time in zip(grid_search.cv_results_[\"params\"], grid_search.cv_results_[\"mean_test_score\"],grid_search.cv_results_[\"std_test_score\"],\n",
    "                                             grid_search.cv_results_[\"rank_test_score\"], grid_search.cv_results_[\"mean_fit_time\"]):\n",
    "        row_item = list(param.values())\n",
    "        row_item.append(score)\n",
    "        row_item.append(std)\n",
    "        row_item.append(rank)\n",
    "        row_item.append(time)\n",
    "        cv_results.append(row_item)\n",
    "        \n",
    "    cv_results = pd.DataFrame(cv_results) \n",
    "    header = cv_results.iloc[0] \n",
    "    cv_results = cv_results[1:]\n",
    "    cv_results = cv_results.rename(columns = header)\n",
    "    cv_results = cv_results.sort_values(by=['rank_test_score'])\n",
    "    cv_results.to_csv(filename)\n",
    "    if print_flag:\n",
    "        print(cv_results.head(6))\n",
    "#        print(cv_results.tail(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = pd .read_csv(\"data/LIWC2015_mod5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = hotels.drop(hotels.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>hotel</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>lc_no_punct</th>\n",
       "      <th>norm</th>\n",
       "      <th>norm_lemma</th>\n",
       "      <th>norm_stem</th>\n",
       "      <th>norm_lemma_stopword</th>\n",
       "      <th>norm_stem_stopword</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>AUX_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>PROPN_count</th>\n",
       "      <th>PUNCT_count</th>\n",
       "      <th>SCONJ_count</th>\n",
       "      <th>SYM_count</th>\n",
       "      <th>VERB_count</th>\n",
       "      <th>X_count</th>\n",
       "      <th>class</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>prep</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>adverb</th>\n",
       "      <th>conj</th>\n",
       "      <th>negate</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "      <th>compare</th>\n",
       "      <th>interrog</th>\n",
       "      <th>number</th>\n",
       "      <th>quant</th>\n",
       "      <th>affect</th>\n",
       "      <th>posemo</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anx</th>\n",
       "      <th>anger</th>\n",
       "      <th>sad</th>\n",
       "      <th>social</th>\n",
       "      <th>family</th>\n",
       "      <th>friend</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>cogproc</th>\n",
       "      <th>insight</th>\n",
       "      <th>cause</th>\n",
       "      <th>discrep</th>\n",
       "      <th>tentat</th>\n",
       "      <th>certain</th>\n",
       "      <th>differ</th>\n",
       "      <th>percept</th>\n",
       "      <th>see</th>\n",
       "      <th>hear</th>\n",
       "      <th>feel</th>\n",
       "      <th>bio</th>\n",
       "      <th>body</th>\n",
       "      <th>health</th>\n",
       "      <th>sexual</th>\n",
       "      <th>ingest</th>\n",
       "      <th>drives</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>achieve</th>\n",
       "      <th>power</th>\n",
       "      <th>reward</th>\n",
       "      <th>risk</th>\n",
       "      <th>focuspast</th>\n",
       "      <th>focuspresent</th>\n",
       "      <th>focusfuture</th>\n",
       "      <th>relativ</th>\n",
       "      <th>motion</th>\n",
       "      <th>space</th>\n",
       "      <th>time</th>\n",
       "      <th>work</th>\n",
       "      <th>leisure</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.00000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>1596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>talbott</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>I'd been searching for a cool, non-chain hotel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>very disappointed in our stay in chicago monoc...</td>\n",
       "      <td>very disappointed in our stay in chicago monoc...</td>\n",
       "      <td>very disappointed in our stay in chicago monoc...</td>\n",
       "      <td>i d be search for a cool non chain hotel for a...</td>\n",
       "      <td>the omni wa chosen for it s locat whichwork ou...</td>\n",
       "      <td>disappointed   stay  chicago monoco   stay ma...</td>\n",
       "      <td>daughter   woke   morn want  go swim   arriv ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>806.391250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099315</td>\n",
       "      <td>0.105465</td>\n",
       "      <td>0.073256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249986</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196686</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>149.373125</td>\n",
       "      <td>71.799387</td>\n",
       "      <td>54.789069</td>\n",
       "      <td>62.433331</td>\n",
       "      <td>66.708150</td>\n",
       "      <td>16.388063</td>\n",
       "      <td>17.162369</td>\n",
       "      <td>88.76245</td>\n",
       "      <td>53.783787</td>\n",
       "      <td>11.534156</td>\n",
       "      <td>7.623800</td>\n",
       "      <td>3.905612</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>0.660881</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>3.895581</td>\n",
       "      <td>9.806800</td>\n",
       "      <td>12.801825</td>\n",
       "      <td>8.915756</td>\n",
       "      <td>5.446219</td>\n",
       "      <td>6.384119</td>\n",
       "      <td>1.677969</td>\n",
       "      <td>15.360131</td>\n",
       "      <td>6.050400</td>\n",
       "      <td>2.071431</td>\n",
       "      <td>0.933763</td>\n",
       "      <td>1.591325</td>\n",
       "      <td>1.943287</td>\n",
       "      <td>5.907506</td>\n",
       "      <td>4.553938</td>\n",
       "      <td>1.12075</td>\n",
       "      <td>0.163806</td>\n",
       "      <td>0.199269</td>\n",
       "      <td>0.203594</td>\n",
       "      <td>7.505700</td>\n",
       "      <td>0.359981</td>\n",
       "      <td>0.442594</td>\n",
       "      <td>0.325388</td>\n",
       "      <td>0.327144</td>\n",
       "      <td>8.955763</td>\n",
       "      <td>1.145400</td>\n",
       "      <td>0.797769</td>\n",
       "      <td>1.566944</td>\n",
       "      <td>1.757069</td>\n",
       "      <td>1.761981</td>\n",
       "      <td>2.854931</td>\n",
       "      <td>2.181400</td>\n",
       "      <td>0.910044</td>\n",
       "      <td>0.462844</td>\n",
       "      <td>0.480525</td>\n",
       "      <td>1.801225</td>\n",
       "      <td>0.252956</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>1.208150</td>\n",
       "      <td>8.111631</td>\n",
       "      <td>3.241900</td>\n",
       "      <td>1.098881</td>\n",
       "      <td>2.213175</td>\n",
       "      <td>1.785962</td>\n",
       "      <td>0.384556</td>\n",
       "      <td>7.265406</td>\n",
       "      <td>6.288981</td>\n",
       "      <td>0.765519</td>\n",
       "      <td>17.468119</td>\n",
       "      <td>2.915563</td>\n",
       "      <td>11.037831</td>\n",
       "      <td>4.889250</td>\n",
       "      <td>2.436375</td>\n",
       "      <td>3.751388</td>\n",
       "      <td>2.768525</td>\n",
       "      <td>1.240731</td>\n",
       "      <td>0.022150</td>\n",
       "      <td>0.020550</td>\n",
       "      <td>0.406875</td>\n",
       "      <td>0.018712</td>\n",
       "      <td>0.043844</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.181806</td>\n",
       "      <td>0.00905</td>\n",
       "      <td>14.055519</td>\n",
       "      <td>6.612544</td>\n",
       "      <td>3.591619</td>\n",
       "      <td>0.087137</td>\n",
       "      <td>0.066175</td>\n",
       "      <td>0.076775</td>\n",
       "      <td>0.767031</td>\n",
       "      <td>0.681212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.280112</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.366150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>467.260647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033587</td>\n",
       "      <td>0.026239</td>\n",
       "      <td>0.027617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.014884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>1.118384</td>\n",
       "      <td>87.739431</td>\n",
       "      <td>18.697766</td>\n",
       "      <td>25.821830</td>\n",
       "      <td>26.023894</td>\n",
       "      <td>34.070231</td>\n",
       "      <td>7.226241</td>\n",
       "      <td>4.382680</td>\n",
       "      <td>4.14512</td>\n",
       "      <td>5.090643</td>\n",
       "      <td>3.915191</td>\n",
       "      <td>3.297535</td>\n",
       "      <td>3.034989</td>\n",
       "      <td>2.542078</td>\n",
       "      <td>1.105959</td>\n",
       "      <td>0.570362</td>\n",
       "      <td>1.037403</td>\n",
       "      <td>1.975704</td>\n",
       "      <td>2.581639</td>\n",
       "      <td>2.861762</td>\n",
       "      <td>2.497208</td>\n",
       "      <td>2.447376</td>\n",
       "      <td>1.966778</td>\n",
       "      <td>1.399532</td>\n",
       "      <td>3.391309</td>\n",
       "      <td>2.658416</td>\n",
       "      <td>1.498134</td>\n",
       "      <td>0.925727</td>\n",
       "      <td>1.507279</td>\n",
       "      <td>1.403120</td>\n",
       "      <td>2.870457</td>\n",
       "      <td>3.081086</td>\n",
       "      <td>1.28789</td>\n",
       "      <td>0.397944</td>\n",
       "      <td>0.464278</td>\n",
       "      <td>0.424580</td>\n",
       "      <td>4.030817</td>\n",
       "      <td>0.734593</td>\n",
       "      <td>0.696959</td>\n",
       "      <td>0.721457</td>\n",
       "      <td>0.650519</td>\n",
       "      <td>3.416747</td>\n",
       "      <td>1.065891</td>\n",
       "      <td>0.839687</td>\n",
       "      <td>1.235211</td>\n",
       "      <td>1.336183</td>\n",
       "      <td>1.384616</td>\n",
       "      <td>1.940218</td>\n",
       "      <td>1.597797</td>\n",
       "      <td>1.086196</td>\n",
       "      <td>0.745820</td>\n",
       "      <td>0.750665</td>\n",
       "      <td>1.694788</td>\n",
       "      <td>0.488442</td>\n",
       "      <td>0.484716</td>\n",
       "      <td>0.091442</td>\n",
       "      <td>1.498268</td>\n",
       "      <td>3.705509</td>\n",
       "      <td>3.115135</td>\n",
       "      <td>1.071992</td>\n",
       "      <td>1.435299</td>\n",
       "      <td>1.474134</td>\n",
       "      <td>0.623480</td>\n",
       "      <td>3.413554</td>\n",
       "      <td>3.136674</td>\n",
       "      <td>0.876047</td>\n",
       "      <td>4.175587</td>\n",
       "      <td>1.561330</td>\n",
       "      <td>3.249295</td>\n",
       "      <td>2.529072</td>\n",
       "      <td>1.699430</td>\n",
       "      <td>2.162818</td>\n",
       "      <td>1.720677</td>\n",
       "      <td>1.269930</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.680934</td>\n",
       "      <td>0.124303</td>\n",
       "      <td>0.211320</td>\n",
       "      <td>0.410125</td>\n",
       "      <td>0.454386</td>\n",
       "      <td>0.08024</td>\n",
       "      <td>5.047531</td>\n",
       "      <td>2.942552</td>\n",
       "      <td>2.465118</td>\n",
       "      <td>0.397933</td>\n",
       "      <td>0.322842</td>\n",
       "      <td>0.351067</td>\n",
       "      <td>1.384438</td>\n",
       "      <td>1.239694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.450216</td>\n",
       "      <td>1.094702</td>\n",
       "      <td>0.850661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.230000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>71.79000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.690000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075330</td>\n",
       "      <td>0.090526</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>60.275000</td>\n",
       "      <td>32.465000</td>\n",
       "      <td>42.347500</td>\n",
       "      <td>35.755000</td>\n",
       "      <td>12.725000</td>\n",
       "      <td>14.167500</td>\n",
       "      <td>86.30000</td>\n",
       "      <td>50.932500</td>\n",
       "      <td>8.890000</td>\n",
       "      <td>5.327500</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>8.107500</td>\n",
       "      <td>10.987500</td>\n",
       "      <td>7.280000</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>13.152500</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.620000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.630000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.095000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.777500</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>8.847500</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>4.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096541</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247807</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>75.015000</td>\n",
       "      <td>54.525000</td>\n",
       "      <td>66.740000</td>\n",
       "      <td>81.230000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>16.985000</td>\n",
       "      <td>89.12500</td>\n",
       "      <td>54.430000</td>\n",
       "      <td>11.585000</td>\n",
       "      <td>7.765000</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>9.785000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>8.940000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>6.320000</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>3.965000</td>\n",
       "      <td>0.82000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.045000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.860000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>2.675000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>7.785000</td>\n",
       "      <td>2.455000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.505000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>10.820000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.395000</td>\n",
       "      <td>6.095000</td>\n",
       "      <td>3.340000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>987.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>0.090109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274336</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>87.052500</td>\n",
       "      <td>77.235000</td>\n",
       "      <td>84.275000</td>\n",
       "      <td>98.870000</td>\n",
       "      <td>18.632500</td>\n",
       "      <td>19.652500</td>\n",
       "      <td>91.55000</td>\n",
       "      <td>57.272500</td>\n",
       "      <td>14.180000</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>5.940000</td>\n",
       "      <td>3.555000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.490000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>7.612500</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>17.687500</td>\n",
       "      <td>7.485000</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>2.352500</td>\n",
       "      <td>2.762500</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>6.435000</td>\n",
       "      <td>1.81000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>10.095000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>1.722500</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>2.590000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.870000</td>\n",
       "      <td>10.470000</td>\n",
       "      <td>5.072500</td>\n",
       "      <td>1.632500</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>9.640000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>20.132500</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>13.060000</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>4.942500</td>\n",
       "      <td>3.722500</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16.670000</td>\n",
       "      <td>7.690000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.015000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.052500</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4159.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.710000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>67.530000</td>\n",
       "      <td>23.810000</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.530000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>8.570000</td>\n",
       "      <td>13.560000</td>\n",
       "      <td>18.450000</td>\n",
       "      <td>22.730000</td>\n",
       "      <td>19.120000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>15.620000</td>\n",
       "      <td>10.670000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>18.030000</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>18.870000</td>\n",
       "      <td>9.38000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>23.810000</td>\n",
       "      <td>5.660000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>7.740000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>23.480000</td>\n",
       "      <td>8.510000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>9.260000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>11.340000</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>6.980000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4.230000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>20.690000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>16.480000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>15.560000</td>\n",
       "      <td>13.240000</td>\n",
       "      <td>7.590000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>1.15000</td>\n",
       "      <td>50.420000</td>\n",
       "      <td>37.820000</td>\n",
       "      <td>26.670000</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>10.640000</td>\n",
       "      <td>12.680000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.450000</td>\n",
       "      <td>8.890000</td>\n",
       "      <td>6.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          deceptive    hotel     polarity source  \\\n",
       "count   1600.000000     1600  1600.000000   1600   \n",
       "unique          NaN       20          NaN      3   \n",
       "top             NaN  talbott          NaN  MTurk   \n",
       "freq            NaN       80          NaN    800   \n",
       "mean       0.500000      NaN     0.500000    NaN   \n",
       "std        0.500156      NaN     0.500156    NaN   \n",
       "min        0.000000      NaN     0.000000    NaN   \n",
       "25%        0.000000      NaN     0.000000    NaN   \n",
       "50%        0.500000      NaN     0.500000    NaN   \n",
       "75%        1.000000      NaN     1.000000    NaN   \n",
       "max        1.000000      NaN     1.000000    NaN   \n",
       "\n",
       "                                                     text  text_length  \\\n",
       "count                                                1600  1600.000000   \n",
       "unique                                               1596          NaN   \n",
       "top     I'd been searching for a cool, non-chain hotel...          NaN   \n",
       "freq                                                    2          NaN   \n",
       "mean                                                  NaN   806.391250   \n",
       "std                                                   NaN   467.260647   \n",
       "min                                                   NaN   151.000000   \n",
       "25%                                                   NaN   487.000000   \n",
       "50%                                                   NaN   700.000000   \n",
       "75%                                                   NaN   987.500000   \n",
       "max                                                   NaN  4159.000000   \n",
       "\n",
       "                                               lower_case  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top     very disappointed in our stay in chicago monoc...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                              lc_no_punct  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top     very disappointed in our stay in chicago monoc...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                                     norm  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top     very disappointed in our stay in chicago monoc...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                               norm_lemma  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top     i d be search for a cool non chain hotel for a...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                                norm_stem  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top     the omni wa chosen for it s locat whichwork ou...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                      norm_lemma_stopword  \\\n",
       "count                                                1600   \n",
       "unique                                               1596   \n",
       "top      disappointed   stay  chicago monoco   stay ma...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                       norm_stem_stopword    ADJ_count  \\\n",
       "count                                                1600  1600.000000   \n",
       "unique                                               1596          NaN   \n",
       "top      daughter   woke   morn want  go swim   arriv ...          NaN   \n",
       "freq                                                    2          NaN   \n",
       "mean                                                  NaN     0.099315   \n",
       "std                                                   NaN     0.033587   \n",
       "min                                                   NaN     0.019048   \n",
       "25%                                                   NaN     0.075330   \n",
       "50%                                                   NaN     0.096541   \n",
       "75%                                                   NaN     0.119300   \n",
       "max                                                   NaN     0.266667   \n",
       "\n",
       "          ADP_count    ADV_count  AUX_count  CCONJ_count    DET_count  \\\n",
       "count   1600.000000  1600.000000     1600.0       1600.0  1600.000000   \n",
       "unique          NaN          NaN        NaN          NaN          NaN   \n",
       "top             NaN          NaN        NaN          NaN          NaN   \n",
       "freq            NaN          NaN        NaN          NaN          NaN   \n",
       "mean       0.105465     0.073256        0.0          0.0     0.124403   \n",
       "std        0.026239     0.027617        0.0          0.0     0.028481   \n",
       "min        0.000000     0.000000        0.0          0.0     0.000000   \n",
       "25%        0.090526     0.054348        0.0          0.0     0.106557   \n",
       "50%        0.106667     0.072289        0.0          0.0     0.124160   \n",
       "75%        0.121527     0.090109        0.0          0.0     0.141747   \n",
       "max        0.192308     0.206522        0.0          0.0     0.236842   \n",
       "\n",
       "        INTJ_count   NOUN_count    NUM_count  PART_count   PRON_count  \\\n",
       "count       1600.0  1600.000000  1600.000000      1600.0  1600.000000   \n",
       "unique         NaN          NaN          NaN         NaN          NaN   \n",
       "top            NaN          NaN          NaN         NaN          NaN   \n",
       "freq           NaN          NaN          NaN         NaN          NaN   \n",
       "mean           0.0     0.249986     0.013510         0.0     0.063546   \n",
       "std            0.0     0.038522     0.014884         0.0     0.030471   \n",
       "min            0.0     0.115942     0.000000         0.0     0.000000   \n",
       "25%            0.0     0.223832     0.000000         0.0     0.041667   \n",
       "50%            0.0     0.247807     0.010363         0.0     0.061377   \n",
       "75%            0.0     0.274336     0.020426         0.0     0.083333   \n",
       "max            0.0     0.418750     0.180328         0.0     0.228571   \n",
       "\n",
       "        PROPN_count  PUNCT_count  SCONJ_count  SYM_count   VERB_count  \\\n",
       "count        1600.0       1600.0       1600.0     1600.0  1600.000000   \n",
       "unique          NaN          NaN          NaN        NaN          NaN   \n",
       "top             NaN          NaN          NaN        NaN          NaN   \n",
       "freq            NaN          NaN          NaN        NaN          NaN   \n",
       "mean            0.0          0.0          0.0        0.0     0.196686   \n",
       "std             0.0          0.0          0.0        0.0     0.034197   \n",
       "min             0.0          0.0          0.0        0.0     0.043478   \n",
       "25%             0.0          0.0          0.0        0.0     0.175676   \n",
       "50%             0.0          0.0          0.0        0.0     0.197346   \n",
       "75%             0.0          0.0          0.0        0.0     0.220000   \n",
       "max             0.0          0.0          0.0        0.0     0.315789   \n",
       "\n",
       "            X_count        class           WC     Analytic        Clout  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.000160     2.500000   149.373125    71.799387    54.789069   \n",
       "std        0.001135     1.118384    87.739431    18.697766    25.821830   \n",
       "min        0.000000     1.000000    25.000000     3.860000     3.480000   \n",
       "25%        0.000000     1.750000    89.000000    60.275000    32.465000   \n",
       "50%        0.000000     2.500000   128.000000    75.015000    54.525000   \n",
       "75%        0.000000     3.250000   183.000000    87.052500    77.235000   \n",
       "max        0.015625     4.000000   784.000000    99.000000    99.000000   \n",
       "\n",
       "          Authentic         Tone          WPS       Sixltr         Dic  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.00000   \n",
       "unique          NaN          NaN          NaN          NaN         NaN   \n",
       "top             NaN          NaN          NaN          NaN         NaN   \n",
       "freq            NaN          NaN          NaN          NaN         NaN   \n",
       "mean      62.433331    66.708150    16.388063    17.162369    88.76245   \n",
       "std       26.023894    34.070231     7.226241     4.382680     4.14512   \n",
       "min        1.000000     1.000000     5.230000     5.450000    71.79000   \n",
       "25%       42.347500    35.755000    12.725000    14.167500    86.30000   \n",
       "50%       66.740000    81.230000    15.400000    16.985000    89.12500   \n",
       "75%       84.275000    98.870000    18.632500    19.652500    91.55000   \n",
       "max       99.000000    99.000000    97.000000    36.710000   100.00000   \n",
       "\n",
       "           function      pronoun        ppron            i           we  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      53.783787    11.534156     7.623800     3.905612     2.055000   \n",
       "std        5.090643     3.915191     3.297535     3.034989     2.542078   \n",
       "min       31.750000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       50.932500     8.890000     5.327500     1.390000     0.000000   \n",
       "50%       54.430000    11.585000     7.765000     3.450000     0.935000   \n",
       "75%       57.272500    14.180000     9.880000     5.940000     3.555000   \n",
       "max       67.530000    23.810000    20.950000    15.000000    12.000000   \n",
       "\n",
       "                you        shehe         they        ipron      article  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.660881     0.218681     0.783550     3.895581     9.806800   \n",
       "std        1.105959     0.570362     1.037403     1.975704     2.581639   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     2.610000     8.107500   \n",
       "50%        0.000000     0.000000     0.490000     3.750000     9.785000   \n",
       "75%        0.980000     0.000000     1.200000     5.000000    11.490000   \n",
       "max        7.530000     6.500000     8.570000    13.560000    18.450000   \n",
       "\n",
       "               prep      auxverb       adverb         conj       negate  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      12.801825     8.915756     5.446219     6.384119     1.677969   \n",
       "std        2.861762     2.497208     2.447376     1.966778     1.399532   \n",
       "min        1.690000     1.090000     0.000000     0.000000     0.000000   \n",
       "25%       10.987500     7.280000     3.730000     5.050000     0.687500   \n",
       "50%       12.850000     8.940000     5.260000     6.320000     1.490000   \n",
       "75%       14.750000    10.370000     6.850000     7.612500     2.500000   \n",
       "max       22.730000    19.120000    17.650000    15.620000    10.670000   \n",
       "\n",
       "               verb          adj      compare     interrog       number  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      15.360131     6.050400     2.071431     0.933763     1.591325   \n",
       "std        3.391309     2.658416     1.498134     0.925727     1.507279   \n",
       "min        2.220000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       13.152500     4.210000     1.020000     0.000000     0.430000   \n",
       "50%       15.380000     5.710000     1.850000     0.800000     1.325000   \n",
       "75%       17.687500     7.485000     2.940000     1.490000     2.352500   \n",
       "max       27.500000    19.050000    10.530000     4.650000    18.030000   \n",
       "\n",
       "              quant       affect       posemo      negemo          anx  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.00000  1600.000000   \n",
       "unique          NaN          NaN          NaN         NaN          NaN   \n",
       "top             NaN          NaN          NaN         NaN          NaN   \n",
       "freq            NaN          NaN          NaN         NaN          NaN   \n",
       "mean       1.943287     5.907506     4.553938     1.12075     0.163806   \n",
       "std        1.403120     2.870457     3.081086     1.28789     0.397944   \n",
       "min        0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "25%        0.967500     3.920000     2.210000     0.00000     0.000000   \n",
       "50%        1.750000     5.490000     3.965000     0.82000     0.000000   \n",
       "75%        2.762500     7.320000     6.435000     1.81000     0.000000   \n",
       "max        8.330000    20.340000    18.870000     9.38000     5.000000   \n",
       "\n",
       "              anger          sad       social       family       friend  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.199269     0.203594     7.505700     0.359981     0.442594   \n",
       "std        0.464278     0.424580     4.030817     0.734593     0.696959   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     4.620000     0.000000     0.000000   \n",
       "50%        0.000000     0.000000     7.045000     0.000000     0.000000   \n",
       "75%        0.000000     0.232500    10.095000     0.510000     0.760000   \n",
       "max        5.560000     3.360000    23.810000     5.660000     5.710000   \n",
       "\n",
       "             female         male      cogproc      insight        cause  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.325388     0.327144     8.955763     1.145400     0.797769   \n",
       "std        0.721457     0.650519     3.416747     1.065891     0.839687   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     6.630000     0.000000     0.000000   \n",
       "50%        0.000000     0.000000     8.860000     1.030000     0.690000   \n",
       "75%        0.352500     0.480000    11.110000     1.722500     1.290000   \n",
       "max        7.740000     5.690000    23.480000     8.510000     4.600000   \n",
       "\n",
       "            discrep       tentat      certain       differ      percept  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       1.566944     1.757069     1.761981     2.854931     2.181400   \n",
       "std        1.235211     1.336183     1.384616     1.940218     1.597797   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.690000     0.797500     0.810000     1.470000     1.100000   \n",
       "50%        1.450000     1.610000     1.540000     2.675000     1.950000   \n",
       "75%        2.290000     2.590000     2.500000     3.980000     3.030000   \n",
       "max        8.000000     8.330000     9.260000    12.500000    11.340000   \n",
       "\n",
       "                see         hear         feel          bio         body  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.910044     0.462844     0.480525     1.801225     0.252956   \n",
       "std        1.086196     0.745820     0.750665     1.694788     0.488442   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     0.577500     0.000000   \n",
       "50%        0.690000     0.000000     0.000000     1.480000     0.000000   \n",
       "75%        1.430000     0.780000     0.800000     2.580000     0.400000   \n",
       "max        8.250000     6.800000     6.980000    12.500000     4.230000   \n",
       "\n",
       "             health       sexual       ingest       drives  affiliation  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.231481     0.010756     1.208150     8.111631     3.241900   \n",
       "std        0.484716     0.091442     1.498268     3.705509     3.115135   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     5.260000     0.617500   \n",
       "50%        0.000000     0.000000     0.790000     7.785000     2.455000   \n",
       "75%        0.320000     0.000000     1.870000    10.470000     5.072500   \n",
       "max        5.130000     1.720000    12.500000    24.000000    15.150000   \n",
       "\n",
       "            achieve        power       reward         risk    focuspast  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       1.098881     2.213175     1.785962     0.384556     7.265406   \n",
       "std        1.071992     1.435299     1.474134     0.623480     3.413554   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     1.230000     0.760000     0.000000     5.000000   \n",
       "50%        0.950000     2.020000     1.520000     0.000000     7.505000   \n",
       "75%        1.632500     3.030000     2.500000     0.702500     9.640000   \n",
       "max       10.530000    10.530000    10.000000     6.060000    18.750000   \n",
       "\n",
       "        focuspresent  focusfuture      relativ       motion        space  \\\n",
       "count    1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique           NaN          NaN          NaN          NaN          NaN   \n",
       "top              NaN          NaN          NaN          NaN          NaN   \n",
       "freq             NaN          NaN          NaN          NaN          NaN   \n",
       "mean        6.288981     0.765519    17.468119     2.915563    11.037831   \n",
       "std         3.136674     0.876047     4.175587     1.561330     3.249295   \n",
       "min         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%         4.095000     0.000000    14.777500     1.830000     8.847500   \n",
       "50%         5.850000     0.590000    17.430000     2.780000    10.820000   \n",
       "75%         8.000000     1.220000    20.132500     3.810000    13.060000   \n",
       "max        20.690000     7.270000    33.330000    10.000000    23.400000   \n",
       "\n",
       "               time         work      leisure         home        money  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       4.889250     2.436375     3.751388     2.768525     1.240731   \n",
       "std        2.529072     1.699430     2.162818     1.720677     1.269930   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        3.080000     1.270000     2.200000     1.590000     0.000000   \n",
       "50%        4.710000     2.150000     3.475000     2.610000     0.970000   \n",
       "75%        6.380000     3.230000     4.942500     3.722500     1.850000   \n",
       "max       16.480000    13.330000    15.560000    13.240000     7.590000   \n",
       "\n",
       "              relig        death     informal        swear     netspeak  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       0.022150     0.020550     0.406875     0.018712     0.043844   \n",
       "std        0.142495     0.137384     0.680934     0.124303     0.211320   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%        0.000000     0.000000     0.680000     0.000000     0.000000   \n",
       "max        2.170000     1.750000     6.250000     2.000000     2.870000   \n",
       "\n",
       "             assent       nonflu      filler      AllPunc       Period  \\\n",
       "count   1600.000000  1600.000000  1600.00000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN         NaN          NaN          NaN   \n",
       "top             NaN          NaN         NaN          NaN          NaN   \n",
       "freq            NaN          NaN         NaN          NaN          NaN   \n",
       "mean       0.149638     0.181806     0.00905    14.055519     6.612544   \n",
       "std        0.410125     0.454386     0.08024     5.047531     2.942552   \n",
       "min        0.000000     0.000000     0.00000     1.540000     0.000000   \n",
       "25%        0.000000     0.000000     0.00000    10.520000     4.960000   \n",
       "50%        0.000000     0.000000     0.00000    13.395000     6.095000   \n",
       "75%        0.000000     0.000000     0.00000    16.670000     7.690000   \n",
       "max        3.920000     6.250000     1.15000    50.420000    37.820000   \n",
       "\n",
       "              Comma        Colon        SemiC        QMark       Exclam  \\\n",
       "count   1600.000000  1600.000000  1600.000000  1600.000000  1600.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean       3.591619     0.087137     0.066175     0.076775     0.767031   \n",
       "std        2.465118     0.397933     0.322842     0.351067     1.384438   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        1.740000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%        3.340000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%        5.130000     0.000000     0.000000     0.000000     1.015000   \n",
       "max       26.670000     8.330000     4.850000     5.450000    10.640000   \n",
       "\n",
       "               Dash   Quote      Apostro      Parenth       OtherP  \n",
       "count   1600.000000  1600.0  1600.000000  1600.000000  1600.000000  \n",
       "unique          NaN     NaN          NaN          NaN          NaN  \n",
       "top             NaN     NaN          NaN          NaN          NaN  \n",
       "freq            NaN     NaN          NaN          NaN          NaN  \n",
       "mean       0.681212     0.0     1.280112     0.526700     0.366150  \n",
       "std        1.239694     0.0     1.450216     1.094702     0.850661  \n",
       "min        0.000000     0.0     0.000000     0.000000     0.000000  \n",
       "25%        0.000000     0.0     0.000000     0.000000     0.000000  \n",
       "50%        0.000000     0.0     0.920000     0.000000     0.000000  \n",
       "75%        1.010000     0.0     2.052500     0.682500     0.380000  \n",
       "max       12.680000     0.0    10.450000     8.890000     6.830000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = hotels.loc[:, \"ADJ_count\":\"X_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>AUX_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>PROPN_count</th>\n",
       "      <th>PUNCT_count</th>\n",
       "      <th>SCONJ_count</th>\n",
       "      <th>SYM_count</th>\n",
       "      <th>VERB_count</th>\n",
       "      <th>X_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADJ_count  ADP_count  ADV_count  AUX_count  CCONJ_count  DET_count  \\\n",
       "0   0.148148   0.120370   0.074074          0            0   0.101852   \n",
       "1   0.136364   0.113636   0.068182          0            0   0.090909   \n",
       "\n",
       "   INTJ_count  NOUN_count  NUM_count  PART_count  PRON_count  PROPN_count  \\\n",
       "0           0    0.277778   0.064815           0    0.037037            0   \n",
       "1           0    0.318182   0.022727           0    0.000000            0   \n",
       "\n",
       "   PUNCT_count  SCONJ_count  SYM_count  VERB_count  X_count  \n",
       "0            0            0          0    0.129630      0.0  \n",
       "1            0            0          0    0.227273      0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ADJ_count', 'ADP_count', 'ADV_count', 'AUX_count', 'CCONJ_count',\n",
       "       'DET_count', 'INTJ_count', 'NOUN_count', 'NUM_count', 'PART_count',\n",
       "       'PRON_count', 'PROPN_count', 'PUNCT_count', 'SCONJ_count', 'SYM_count',\n",
       "       'VERB_count', 'X_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Choose input data and target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hotels[\"deceptive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Split data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Build pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_lr = [\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', LogisticRegression()),\n",
    "]\n",
    "\n",
    "pipe_lr = Pipeline(steps_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Construct parameters for cross validation testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = [\n",
    "    {\n",
    "         'clf__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Apply Grid Serach CV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.254s\n",
      "best params:\n",
      "{'clf__C': 0.01}\n",
      "Best cross-validation score: 0.693\n",
      "Test-set score: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "log_reg_grid_search = apply_grid_search_cv(pipe_lr, param_grid_lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Save Classification Report to File__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truthful</th>\n",
       "      <td>0.618510</td>\n",
       "      <td>0.674877</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deceptive</th>\n",
       "      <td>0.673114</td>\n",
       "      <td>0.628159</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.647917</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.645812</td>\n",
       "      <td>0.651518</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.645812</td>\n",
       "      <td>0.651518</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1-score  precision    recall  support\n",
       "truthful      0.618510   0.674877  0.570833    240.0\n",
       "deceptive     0.673114   0.628159  0.725000    240.0\n",
       "micro avg     0.647917   0.647917  0.647917    480.0\n",
       "macro avg     0.645812   0.651518  0.647917    480.0\n",
       "weighted avg  0.645812   0.651518  0.647917    480.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_class_report_cv(log_reg_grid_search, X_test, y_test, [\"truthful\", \"deceptive\"], \"output/log_reg_pos1_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clf__C mean_test_score std_test_score rank_test_score mean_fit_time\n",
      "2   0.01        0.693191      0.0057881               1     0.0113692\n",
      "3    0.1         0.68871     0.00565216               2    0.00977316\n",
      "4      1        0.688162     0.00646212               3    0.00917616\n",
      "5     10        0.688162     0.00646212               3    0.00937433\n",
      "6    100        0.688162     0.00646212               3    0.00937481\n",
      "1  0.001        0.675549       0.020399               6     0.0233385\n"
     ]
    }
   ],
   "source": [
    "save_cross_validation_results(log_reg_grid_search, \"output/log_reg_pos1_validation_res.csv\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Save model to file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/log_reg_pos1.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(log_reg_grid_search.best_estimator_, 'output/log_reg_pos1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7b. Identify important features for deceptive opinion spam__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest scaled value:\n",
      "Index(['AUX_count', 'CCONJ_count', 'INTJ_count', 'PART_count', 'PROPN_count',\n",
      "       'PUNCT_count', 'SCONJ_count', 'SYM_count', 'ADP_count', 'VERB_count'],\n",
      "      dtype='object')\n",
      "Features with highest scaled value: \n",
      "Index(['SYM_count', 'ADP_count', 'VERB_count', 'PRON_count', 'DET_count',\n",
      "       'NOUN_count', 'ADV_count', 'ADJ_count', 'NUM_count', 'X_count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "scaler = log_reg_grid_search.best_estimator_.named_steps[\"scale\"]\n",
    "\n",
    "# transform the training dataset\n",
    "X_train1 = scaler.transform(X_train)\n",
    "\n",
    "max_value = X_train1.max(axis=0).ravel()\n",
    "sorted_by_scale = max_value.argsort()\n",
    "\n",
    "#feature_names = np.array(scaler.get_feature_names())\n",
    "feature_names = X_train.columns\n",
    "\n",
    "print(\"Features with lowest scaled value:\\n{}\".format(\n",
    "    feature_names[sorted_by_scale[:10]]))\n",
    "\n",
    "print(\"Features with highest scaled value: \\n{}\".format(\n",
    "    feature_names[sorted_by_scale[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.visualize_coefficients(\n",
    "    log_reg_grid_search.best_estimator_.named_steps[\"clf\"].coef_,\n",
    "    feature_names, n_top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ The negative coefficients on the left represent truthful reviews (as it is negative of the \"deceptive\" column) while the positive coefficients on the right represent the deceptive reviews. From here we can see that truthful reviews have more numerical values, determiners / articles and adjective. The deceptive reviews on the other hand have more verbs, adverbs, pronouns, adposition and nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Sentiment Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.239s\n",
      "best params:\n",
      "{'clf__C': 0.01}\n",
      "Best cross-validation score: 0.704\n",
      "Test-set score: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "y = hotels[\"polarity\"]\n",
    "X_train, X_test, y_train, y_test = split_train_test_data(X, y)\n",
    "log_reg_grid_search_p = apply_grid_search_cv(pipe_lr, param_grid_lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clf__C mean_test_score std_test_score rank_test_score mean_fit_time\n",
      "2   0.01        0.703667      0.0279262               1     0.0131663\n",
      "4      1        0.702497      0.0324811               2     0.0137667\n",
      "5     10        0.701797      0.0315768               3    0.00537796\n",
      "6    100        0.701797      0.0315768               3    0.00897503\n",
      "3    0.1        0.700162        0.02677               5     0.0209448\n",
      "1  0.001        0.692006      0.0290417               6     0.0107713\n"
     ]
    }
   ],
   "source": [
    "save_cross_validation_results(log_reg_grid_search_p, \"output/log_reg_pos_sent_validation_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.717213</td>\n",
       "      <td>0.705645</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.707627</td>\n",
       "      <td>0.719828</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.712420</td>\n",
       "      <td>0.712736</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.712420</td>\n",
       "      <td>0.712736</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1-score  precision    recall  support\n",
       "negative      0.717213   0.705645  0.729167    240.0\n",
       "positive      0.707627   0.719828  0.695833    240.0\n",
       "micro avg     0.712500   0.712500  0.712500    480.0\n",
       "macro avg     0.712420   0.712736  0.712500    480.0\n",
       "weighted avg  0.712420   0.712736  0.712500    480.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_class_report_cv(log_reg_grid_search_p, X_test, y_test, [\"negative\", \"positive\"], \"output/log_reg_pos_report_sent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8b. Identify important features for sentiment analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest scaled value:\n",
      "Index(['AUX_count', 'CCONJ_count', 'INTJ_count', 'PART_count', 'PROPN_count',\n",
      "       'PUNCT_count', 'SCONJ_count', 'SYM_count', 'ADP_count', 'VERB_count'],\n",
      "      dtype='object')\n",
      "Features with highest scaled value: \n",
      "Index(['SYM_count', 'ADP_count', 'VERB_count', 'NOUN_count', 'DET_count',\n",
      "       'ADV_count', 'ADJ_count', 'PRON_count', 'NUM_count', 'X_count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "scaler = log_reg_grid_search_p.best_estimator_.named_steps[\"scale\"]\n",
    "\n",
    "# transform the training dataset\n",
    "X_train1 = scaler.transform(X_train)\n",
    "\n",
    "max_value = X_train1.max(axis=0).ravel()\n",
    "sorted_by_scale = max_value.argsort()\n",
    "\n",
    "#feature_names = np.array(scaler.get_feature_names())\n",
    "feature_names = X_train.columns\n",
    "\n",
    "print(\"Features with lowest scaled value:\\n{}\".format(\n",
    "    feature_names[sorted_by_scale[:10]]))\n",
    "\n",
    "print(\"Features with highest scaled value: \\n{}\".format(\n",
    "    feature_names[sorted_by_scale[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.visualize_coefficients(\n",
    "    log_reg_grid_search_p.best_estimator_.named_steps[\"clf\"].coef_,\n",
    "    feature_names, n_top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ The negative coefficients on the left represent negative sentiments while the positive coefficients on the right represent positive sentiments. It appears that negative reviews contain more verb, adposition, advertbs and numbers while the positive reviews contain more adjectives and nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Combined TFIDF with POS Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADP_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>AUX_count</th>\n",
       "      <th>CCONJ_count</th>\n",
       "      <th>DET_count</th>\n",
       "      <th>INTJ_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>NUM_count</th>\n",
       "      <th>PART_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>PROPN_count</th>\n",
       "      <th>PUNCT_count</th>\n",
       "      <th>SCONJ_count</th>\n",
       "      <th>SYM_count</th>\n",
       "      <th>VERB_count</th>\n",
       "      <th>X_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we stayed for a one night getaway with family ...</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triple a rate with upgrade to view room was le...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                norm  ADJ_count  ADP_count  \\\n",
       "0  we stayed for a one night getaway with family ...   0.148148   0.120370   \n",
       "1  triple a rate with upgrade to view room was le...   0.136364   0.113636   \n",
       "\n",
       "   ADV_count  AUX_count  CCONJ_count  DET_count  INTJ_count  NOUN_count  \\\n",
       "0   0.074074          0            0   0.101852           0    0.277778   \n",
       "1   0.068182          0            0   0.090909           0    0.318182   \n",
       "\n",
       "   NUM_count  PART_count  PRON_count  PROPN_count  PUNCT_count  SCONJ_count  \\\n",
       "0   0.064815           0    0.037037            0            0            0   \n",
       "1   0.022727           0    0.000000            0            0            0   \n",
       "\n",
       "   SYM_count  VERB_count  X_count  \n",
       "0          0    0.129630      0.0  \n",
       "1          0    0.227273      0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels_comb = pd.concat([hotels[\"norm\"], pos], axis=1)\n",
    "hotels_comb.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Choose input data and target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hotels_comb\n",
    "y = hotels[\"deceptive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Split data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Build pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Construct parameters for cross validation testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr_comb = [\n",
    "    {\n",
    "         'clf__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemExcluder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_frame):\n",
    "        #df = data_frame.loc[:, data_frame.columns != self.key]\n",
    "        df = data_frame.drop([self.key], axis=1)\n",
    "        return df\n",
    "    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_frame):\n",
    "        df = data_frame[self.key]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_combined = [\n",
    "    ('preprocess', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('terms', Pipeline([\n",
    "                ('selector', ItemSelector(key='norm')),\n",
    "                ('vect', CountVectorizer()),                \n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('pos', Pipeline([\n",
    "                ('selector', ItemExcluder(key='norm')),\n",
    "                ('scale', StandardScaler())\n",
    "            ]))                        \n",
    "        ]    \n",
    "    )),        \n",
    "    ('clf', LogisticRegression())\n",
    "]\n",
    "\n",
    "pipe_comb = Pipeline(steps_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_comb = [\n",
    "    {'preprocess__terms__vect__stop_words': ['english', None],\n",
    "        'preprocess__terms__vect__min_df': [1, 2, 5], \n",
    "        'preprocess__terms__vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "         'clf__C': [0.1, 1, 10, 100, 1000]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Apply Grid Serach CV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 76.269s\n",
      "best params:\n",
      "{'clf__C': 1000, 'preprocess__terms__vect__min_df': 2, 'preprocess__terms__vect__ngram_range': (1, 2), 'preprocess__terms__vect__stop_words': None}\n",
      "Best cross-validation score: 0.867\n",
      "Test-set score: 0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "lr_pos_comb_grid_search = apply_grid_search_cv(pipe_comb, param_grid_comb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Save Reports to File__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   clf__C preprocess__terms__vect__min_df  \\\n",
      "82   1000                               2   \n",
      "79   1000                               2   \n",
      "64    100                               2   \n",
      "73   1000                               1   \n",
      "84   1000                               2   \n",
      "81   1000                               2   \n",
      "\n",
      "   preprocess__terms__vect__ngram_range preprocess__terms__vect__stop_words  \\\n",
      "82                               (1, 2)                                None   \n",
      "79                               (1, 1)                             english   \n",
      "64                               (1, 2)                                None   \n",
      "73                               (1, 1)                             english   \n",
      "84                               (1, 3)                                None   \n",
      "81                               (1, 2)                             english   \n",
      "\n",
      "   mean_test_score std_test_score rank_test_score mean_fit_time  \n",
      "82        0.867344       0.021772               1      0.983173  \n",
      "79        0.866312      0.0204015               2       0.28155  \n",
      "64        0.866229      0.0187537               3      0.912764  \n",
      "73        0.865753      0.0275363               4      0.278456  \n",
      "84        0.864654       0.023391               5       1.61553  \n",
      "81        0.863977      0.0221407               6      0.553518  \n"
     ]
    }
   ],
   "source": [
    "save_cross_validation_results(lr_pos_comb_grid_search, \"output/lr_pos_comb1_validation_res.csv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Save model to file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truthful</th>\n",
       "      <td>0.868817</td>\n",
       "      <td>0.897778</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deceptive</th>\n",
       "      <td>0.876768</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.872917</td>\n",
       "      <td>0.872917</td>\n",
       "      <td>0.872917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.872792</td>\n",
       "      <td>0.874379</td>\n",
       "      <td>0.872917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.872792</td>\n",
       "      <td>0.874379</td>\n",
       "      <td>0.872917</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1-score  precision    recall  support\n",
       "truthful      0.868817   0.897778  0.841667    240.0\n",
       "deceptive     0.876768   0.850980  0.904167    240.0\n",
       "micro avg     0.872917   0.872917  0.872917    480.0\n",
       "macro avg     0.872792   0.874379  0.872917    480.0\n",
       "weighted avg  0.872792   0.874379  0.872917    480.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_class_report_cv(lr_pos_comb_grid_search, X_test, y_test, [\"truthful\", \"deceptive\"], \"output/lr_pos_comb1_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Sentiment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 81.215s\n",
      "best params:\n",
      "{'clf__C': 1000, 'preprocess__terms__vect__min_df': 2, 'preprocess__terms__vect__ngram_range': (1, 1), 'preprocess__terms__vect__stop_words': None}\n",
      "Best cross-validation score: 0.944\n",
      "Test-set score: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:465: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "y = hotels[\"polarity\"]\n",
    "X_train, X_test, y_train, y_test = split_train_test_data(X, y)\n",
    "lr_pos_comb_grid_search_p = apply_grid_search_cv(pipe_comb, param_grid_comb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   clf__C preprocess__terms__vect__min_df  \\\n",
      "80   1000                               2   \n",
      "90   1000                               5   \n",
      "62    100                               2   \n",
      "70    100                               5   \n",
      "88   1000                               5   \n",
      "72    100                               5   \n",
      "\n",
      "   preprocess__terms__vect__ngram_range preprocess__terms__vect__stop_words  \\\n",
      "80                               (1, 1)                                None   \n",
      "90                               (1, 3)                                None   \n",
      "62                               (1, 1)                                None   \n",
      "70                               (1, 2)                                None   \n",
      "88                               (1, 2)                                None   \n",
      "72                               (1, 3)                                None   \n",
      "\n",
      "   mean_test_score std_test_score rank_test_score mean_fit_time  \n",
      "80        0.943601      0.0113388               1      0.418283  \n",
      "90        0.942819      0.0113456               2       1.46172  \n",
      "62        0.942753      0.0112997               3      0.371816  \n",
      "70        0.942663      0.0124231               4      0.908572  \n",
      "88        0.941814      0.0109514               5      0.792686  \n",
      "72        0.941786      0.0118809               6       1.85865  \n"
     ]
    }
   ],
   "source": [
    "save_cross_validation_results(lr_pos_comb_grid_search_p, \"output/lr_pos_comb1_validation_res_sent.csv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susan\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.952965</td>\n",
       "      <td>0.935743</td>\n",
       "      <td>0.970833</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.951168</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.952066</td>\n",
       "      <td>0.952720</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.952066</td>\n",
       "      <td>0.952720</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1-score  precision    recall  support\n",
       "negative      0.952965   0.935743  0.970833    240.0\n",
       "positive      0.951168   0.969697  0.933333    240.0\n",
       "micro avg     0.952083   0.952083  0.952083    480.0\n",
       "macro avg     0.952066   0.952720  0.952083    480.0\n",
       "weighted avg  0.952066   0.952720  0.952083    480.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_class_report_cv(lr_pos_comb_grid_search_p, X_test, y_test, [\"negative\", \"positive\"], \"output/lr_pos_comb1_report_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
